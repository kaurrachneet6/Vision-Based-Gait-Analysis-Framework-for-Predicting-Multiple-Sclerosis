{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### 1D Convolutional neural network (CNN) on task generalization framework 1: train on walking (W) and test on walking while talking (WT) to classify HOA/MS/PD strides and subjects \n",
    "#### Remember to add the original count of frames in a single stride (before down sampling via smoothing) for each stride as an additional artificial feature to add information about speed of the subject to the model\n",
    "\n",
    "1. Save the optimal hyperparameters, confusion matrices and ROC curves for each algorithm.\n",
    "2. Make sure to not use x, y, z, confidence = 0, 0, 0, 0 as points for the model since they are simply missing values and not data points, so make sure to treat them before inputting to model \n",
    "3. Make sure to normalize (mean substract) the features before we feed them to the model.\n",
    "4. Make sure to set a random seed wherever required for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import imports \n",
    "reload(imports)\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed_value, use_cuda):\n",
    "    '''\n",
    "    To set the random seed for reproducibility of results \n",
    "    Arguments: seed value and use cuda (True if cuda is available)\n",
    "    '''\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subjects_common_across_train_test(trial_train, trial_test):\n",
    "    '''\n",
    "    Since we need to implement pure task generalization framework, we must have same subjects across both training and testing trails \n",
    "    Hence, if there are some subjects that are present in the training set but not in the test set or vice versa, we eliminate those \n",
    "    subjects to have only common subjects across training and test sets. \n",
    "    Arguments: data subset for training and testing trial\n",
    "    Returns: PIDs to retain in the training and test subsets with common subjects \n",
    "    '''\n",
    "    \n",
    "    print ('Original number of subjects in training and test sets:', len(trial_train['PID'].unique()), len(trial_test['PID'].unique()))\n",
    "\n",
    "    #Try to use same subjects in trials W and WT for testing on same subjects we train on\n",
    "    print ('Subjects in test set, which are not in training set')\n",
    "    pids_missing_training = [] #PIDs missing in training set (trial W) but are present in the test set (trial WT)\n",
    "    for x in trial_test['PID'].unique():\n",
    "        if x not in trial_train['PID'].unique():\n",
    "            pids_missing_training.append(x)\n",
    "    print (pids_missing_training)\n",
    "    #List of PIDs to retain in the training set \n",
    "    pids_retain_training = [i for i in trial_test['PID'].unique() if i not in pids_missing_training]\n",
    "    \n",
    "    print ('Subjects in training set, which are not in test set')\n",
    "    pids_missing_test = [] #PIDs missing in test set (trial WT) but are present in the training set (trial W)\n",
    "    for x in trial_train['PID'].unique():\n",
    "        if x not in trial_test['PID'].unique():\n",
    "            pids_missing_test.append(x)\n",
    "    print (pids_missing_test)\n",
    "    #List of PIDs to retain in the testing set \n",
    "    pids_retain_test = [i for i in trial_train['PID'].unique() if i not in pids_missing_test]\n",
    "    \n",
    "    print ('Number of subjects in training and test sets after reduction:', len(pids_retain_training), \\\n",
    "           len(pids_retain_test))\n",
    "    #Returning the PIDs to retain in the training and test set\n",
    "    return  pids_retain_training, pids_retain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data before ML methods \n",
    "#Take care that testing set is not used while normalizaing the training set, otherwise the train set indirectly contains \n",
    "#information about the test set\n",
    "def normalize(dataframe, n_type): \n",
    "    '''\n",
    "    Arguments: training set dataframe, type of normalization (z-score or min-max)\n",
    "    Returns: Computed mean and standard deviation for the training set \n",
    "    '''\n",
    "    col_names = list(dataframe.columns)\n",
    "    if (n_type == 'z'): #z-score normalization \n",
    "        mean = dataframe.mean()\n",
    "        sd = dataframe.std()\n",
    "    else: #min-max normalization\n",
    "        mean = dataframe.min()\n",
    "        sd = dataframe.max()-dataframe.min()\n",
    "    return mean, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch dataset definition\n",
    "class GaitDataset(Dataset):\n",
    "    #We need to add the frame count as an extra feature along with 36 features for each stride \n",
    "    def __init__(self, data_path, labels_csv, pids_retain, framework = 'W'):   \n",
    "        '''\n",
    "        Arguments: \n",
    "        data_path: data path for downsampled strides \n",
    "        labels_csv: csv file with labels \n",
    "        pids_retain: PIDs to return data for \n",
    "        framework: Task to return data for \n",
    "        \n",
    "        Returns:\n",
    "        X: 20 rows for 20 downsampled frames per stride and 37 columns for 37 features for each sample\n",
    "        y: PID and label for each sample\n",
    "        '''\n",
    "        #Assigning the data folder for the downsampled strides \n",
    "        self.data_path = data_path\n",
    "        #Reading the labels file\n",
    "        self.all_labels = pd.read_csv(labels_csv, index_col = 0)\n",
    "        #Retaining only the labels dataframe for framework and PIDs of interest and resetting the index\n",
    "        self.reduced_labels = self.labels[self.labels.scenario == framework][self.labels.PID.isin(pids_retain)].reset_index()\n",
    "        #Setting the labels with index as the key and PID along with to use when computing subject wise evaluation metrics\n",
    "        self.labels = self.reduced_labels[['PID', 'label', 'key']].set_index('key')\n",
    "        self.len = len(self.labels) #Length of the data to use\n",
    "    \n",
    "    def __len__(self):\n",
    "        #Returns the length of the data \n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Generates one sample of data\n",
    "        #Select key to sample\n",
    "        key = self.reduced_labels['key'].iloc[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = pd.read_csv(data_path+key+'.csv', index_col = 0)\n",
    "        #Creating a new frame count column represting the total original count of frames in a stride \n",
    "        #denoting the speed of the stride\n",
    "        X['frame_count'] = self.reduced_labels[self.reduced_labels['key']==key]['frame_count'].values[0]\n",
    "        y = self.labels.loc[key] #PID and label extracted for the key at the index \n",
    "        #X- 20 rows for 20 downsampled frames per stride and 37 columns for 37 features for each sample\n",
    "        #y - PID and label for each sample\n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PID      212\n",
       "label      0\n",
       "Name: GVS_212_W_T2_2, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_csv(labels_file, index_col = 0)\n",
    "x[x.scenario == 'W'][x.PID.isin(pids_retain_trialWT)].reset_index()[['PID', 'label', 'key']].set_index('key').loc['GVS_212_W_T2_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[x.scenario == 'W'][x.PID.isin(pids_retain_trialWT)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y['key']=='GVS_212_W_T2_2']['frame_count'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right hip-x</th>\n",
       "      <th>right hip-y</th>\n",
       "      <th>right hip-z</th>\n",
       "      <th>right knee-x</th>\n",
       "      <th>right knee-y</th>\n",
       "      <th>right knee-z</th>\n",
       "      <th>right ankle-x</th>\n",
       "      <th>right ankle-y</th>\n",
       "      <th>right ankle-z</th>\n",
       "      <th>left hip-x</th>\n",
       "      <th>...</th>\n",
       "      <th>right toe 1-x</th>\n",
       "      <th>right toe 1-y</th>\n",
       "      <th>right toe 1-z</th>\n",
       "      <th>right toe 2-x</th>\n",
       "      <th>right toe 2-y</th>\n",
       "      <th>right toe 2-z</th>\n",
       "      <th>right heel-x</th>\n",
       "      <th>right heel-y</th>\n",
       "      <th>right heel-z</th>\n",
       "      <th>frame_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.719182</td>\n",
       "      <td>132.655828</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.325270</td>\n",
       "      <td>144.253953</td>\n",
       "      <td>86.735232</td>\n",
       "      <td>34.595294</td>\n",
       "      <td>153.503292</td>\n",
       "      <td>15.948365</td>\n",
       "      <td>14.023882</td>\n",
       "      <td>...</td>\n",
       "      <td>35.242309</td>\n",
       "      <td>169.563125</td>\n",
       "      <td>4.569527</td>\n",
       "      <td>40.288776</td>\n",
       "      <td>167.144811</td>\n",
       "      <td>4.858135</td>\n",
       "      <td>33.377358</td>\n",
       "      <td>158.475119</td>\n",
       "      <td>12.045356</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.704212</td>\n",
       "      <td>132.073088</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.919103</td>\n",
       "      <td>143.095944</td>\n",
       "      <td>94.600647</td>\n",
       "      <td>33.259860</td>\n",
       "      <td>144.370975</td>\n",
       "      <td>23.746151</td>\n",
       "      <td>13.610768</td>\n",
       "      <td>...</td>\n",
       "      <td>33.982025</td>\n",
       "      <td>164.228804</td>\n",
       "      <td>10.583161</td>\n",
       "      <td>39.392570</td>\n",
       "      <td>162.468908</td>\n",
       "      <td>11.766435</td>\n",
       "      <td>30.523745</td>\n",
       "      <td>149.269797</td>\n",
       "      <td>20.490017</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.757150</td>\n",
       "      <td>133.531734</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.927445</td>\n",
       "      <td>143.484909</td>\n",
       "      <td>99.527298</td>\n",
       "      <td>33.742680</td>\n",
       "      <td>150.392426</td>\n",
       "      <td>28.619502</td>\n",
       "      <td>12.970735</td>\n",
       "      <td>...</td>\n",
       "      <td>35.799198</td>\n",
       "      <td>164.162529</td>\n",
       "      <td>16.287762</td>\n",
       "      <td>40.448978</td>\n",
       "      <td>161.393837</td>\n",
       "      <td>18.137520</td>\n",
       "      <td>32.021778</td>\n",
       "      <td>154.927910</td>\n",
       "      <td>24.901434</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.442515</td>\n",
       "      <td>134.717514</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.957581</td>\n",
       "      <td>144.659558</td>\n",
       "      <td>99.949475</td>\n",
       "      <td>30.496209</td>\n",
       "      <td>155.566089</td>\n",
       "      <td>33.676301</td>\n",
       "      <td>12.325353</td>\n",
       "      <td>...</td>\n",
       "      <td>33.365414</td>\n",
       "      <td>175.780893</td>\n",
       "      <td>23.020745</td>\n",
       "      <td>37.705657</td>\n",
       "      <td>172.479690</td>\n",
       "      <td>22.709054</td>\n",
       "      <td>29.137211</td>\n",
       "      <td>159.996572</td>\n",
       "      <td>30.494509</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.396707</td>\n",
       "      <td>133.543384</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.218107</td>\n",
       "      <td>143.900380</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>33.292064</td>\n",
       "      <td>157.982605</td>\n",
       "      <td>30.920655</td>\n",
       "      <td>11.515416</td>\n",
       "      <td>...</td>\n",
       "      <td>37.023137</td>\n",
       "      <td>165.521434</td>\n",
       "      <td>21.643583</td>\n",
       "      <td>40.332220</td>\n",
       "      <td>158.163000</td>\n",
       "      <td>25.219125</td>\n",
       "      <td>33.715608</td>\n",
       "      <td>162.032827</td>\n",
       "      <td>26.092724</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44.011256</td>\n",
       "      <td>135.634908</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.921592</td>\n",
       "      <td>146.247061</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30.310831</td>\n",
       "      <td>167.000969</td>\n",
       "      <td>34.354335</td>\n",
       "      <td>10.494606</td>\n",
       "      <td>...</td>\n",
       "      <td>35.097401</td>\n",
       "      <td>181.383237</td>\n",
       "      <td>22.829052</td>\n",
       "      <td>38.631187</td>\n",
       "      <td>177.127592</td>\n",
       "      <td>23.021628</td>\n",
       "      <td>30.074487</td>\n",
       "      <td>169.837089</td>\n",
       "      <td>30.686433</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.821140</td>\n",
       "      <td>135.394196</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.245708</td>\n",
       "      <td>144.679104</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>31.755593</td>\n",
       "      <td>148.063947</td>\n",
       "      <td>36.570477</td>\n",
       "      <td>9.085493</td>\n",
       "      <td>...</td>\n",
       "      <td>33.757653</td>\n",
       "      <td>169.501270</td>\n",
       "      <td>24.399920</td>\n",
       "      <td>38.071680</td>\n",
       "      <td>165.887495</td>\n",
       "      <td>24.902200</td>\n",
       "      <td>30.416170</td>\n",
       "      <td>151.612852</td>\n",
       "      <td>33.927917</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41.706588</td>\n",
       "      <td>132.164874</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.376977</td>\n",
       "      <td>148.178400</td>\n",
       "      <td>98.631682</td>\n",
       "      <td>35.045406</td>\n",
       "      <td>133.591160</td>\n",
       "      <td>38.300539</td>\n",
       "      <td>6.035339</td>\n",
       "      <td>...</td>\n",
       "      <td>35.981628</td>\n",
       "      <td>158.822261</td>\n",
       "      <td>21.575871</td>\n",
       "      <td>40.596812</td>\n",
       "      <td>155.486118</td>\n",
       "      <td>23.801726</td>\n",
       "      <td>32.160150</td>\n",
       "      <td>134.193325</td>\n",
       "      <td>37.892515</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41.450959</td>\n",
       "      <td>135.106239</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.346177</td>\n",
       "      <td>147.794723</td>\n",
       "      <td>97.464171</td>\n",
       "      <td>36.340191</td>\n",
       "      <td>139.477843</td>\n",
       "      <td>32.979997</td>\n",
       "      <td>6.524072</td>\n",
       "      <td>...</td>\n",
       "      <td>39.200288</td>\n",
       "      <td>167.307636</td>\n",
       "      <td>14.538339</td>\n",
       "      <td>43.801626</td>\n",
       "      <td>163.451408</td>\n",
       "      <td>17.108954</td>\n",
       "      <td>32.771995</td>\n",
       "      <td>141.165296</td>\n",
       "      <td>31.850373</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>41.128020</td>\n",
       "      <td>136.576027</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.355909</td>\n",
       "      <td>150.163014</td>\n",
       "      <td>94.819617</td>\n",
       "      <td>38.229298</td>\n",
       "      <td>160.915547</td>\n",
       "      <td>17.701734</td>\n",
       "      <td>6.544273</td>\n",
       "      <td>...</td>\n",
       "      <td>42.358943</td>\n",
       "      <td>183.884977</td>\n",
       "      <td>2.486237</td>\n",
       "      <td>47.355743</td>\n",
       "      <td>181.081490</td>\n",
       "      <td>4.360121</td>\n",
       "      <td>34.786343</td>\n",
       "      <td>165.851328</td>\n",
       "      <td>14.418940</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41.127840</td>\n",
       "      <td>136.283917</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.387879</td>\n",
       "      <td>153.316648</td>\n",
       "      <td>99.882976</td>\n",
       "      <td>33.484270</td>\n",
       "      <td>188.491365</td>\n",
       "      <td>24.726870</td>\n",
       "      <td>5.479543</td>\n",
       "      <td>...</td>\n",
       "      <td>39.542642</td>\n",
       "      <td>206.101765</td>\n",
       "      <td>10.688838</td>\n",
       "      <td>43.034060</td>\n",
       "      <td>203.296841</td>\n",
       "      <td>10.296150</td>\n",
       "      <td>33.042080</td>\n",
       "      <td>191.983551</td>\n",
       "      <td>20.804520</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40.804036</td>\n",
       "      <td>137.173837</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.089713</td>\n",
       "      <td>155.274678</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>32.933592</td>\n",
       "      <td>195.638889</td>\n",
       "      <td>26.376265</td>\n",
       "      <td>5.150223</td>\n",
       "      <td>...</td>\n",
       "      <td>36.059611</td>\n",
       "      <td>213.511911</td>\n",
       "      <td>14.582165</td>\n",
       "      <td>40.567990</td>\n",
       "      <td>211.099269</td>\n",
       "      <td>13.447851</td>\n",
       "      <td>32.667632</td>\n",
       "      <td>197.475320</td>\n",
       "      <td>22.586241</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>41.962157</td>\n",
       "      <td>139.488308</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.397172</td>\n",
       "      <td>159.122662</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>35.857686</td>\n",
       "      <td>196.225231</td>\n",
       "      <td>19.031843</td>\n",
       "      <td>6.340591</td>\n",
       "      <td>...</td>\n",
       "      <td>37.237976</td>\n",
       "      <td>214.995300</td>\n",
       "      <td>6.612093</td>\n",
       "      <td>42.763318</td>\n",
       "      <td>212.886866</td>\n",
       "      <td>6.580670</td>\n",
       "      <td>33.770535</td>\n",
       "      <td>201.036066</td>\n",
       "      <td>15.520785</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44.814544</td>\n",
       "      <td>137.956313</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.917605</td>\n",
       "      <td>161.034622</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>35.491095</td>\n",
       "      <td>197.132328</td>\n",
       "      <td>21.234687</td>\n",
       "      <td>7.702167</td>\n",
       "      <td>...</td>\n",
       "      <td>36.607917</td>\n",
       "      <td>219.315474</td>\n",
       "      <td>6.192468</td>\n",
       "      <td>41.517895</td>\n",
       "      <td>217.381384</td>\n",
       "      <td>6.172916</td>\n",
       "      <td>33.110456</td>\n",
       "      <td>201.036807</td>\n",
       "      <td>18.502766</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45.549744</td>\n",
       "      <td>137.644661</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.967553</td>\n",
       "      <td>161.810606</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30.285618</td>\n",
       "      <td>200.611370</td>\n",
       "      <td>27.327022</td>\n",
       "      <td>8.404307</td>\n",
       "      <td>...</td>\n",
       "      <td>36.031930</td>\n",
       "      <td>217.237944</td>\n",
       "      <td>12.071057</td>\n",
       "      <td>36.694410</td>\n",
       "      <td>216.346899</td>\n",
       "      <td>10.551490</td>\n",
       "      <td>29.971744</td>\n",
       "      <td>202.996275</td>\n",
       "      <td>24.731801</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46.328729</td>\n",
       "      <td>139.386306</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.327667</td>\n",
       "      <td>161.808180</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>32.110433</td>\n",
       "      <td>196.302087</td>\n",
       "      <td>26.165533</td>\n",
       "      <td>9.204564</td>\n",
       "      <td>...</td>\n",
       "      <td>36.071895</td>\n",
       "      <td>219.783631</td>\n",
       "      <td>3.718340</td>\n",
       "      <td>41.130656</td>\n",
       "      <td>217.793346</td>\n",
       "      <td>4.825427</td>\n",
       "      <td>28.199790</td>\n",
       "      <td>199.837905</td>\n",
       "      <td>24.909436</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>48.574329</td>\n",
       "      <td>140.505220</td>\n",
       "      <td>100.0</td>\n",
       "      <td>42.930678</td>\n",
       "      <td>165.276049</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.834669</td>\n",
       "      <td>189.358980</td>\n",
       "      <td>18.526370</td>\n",
       "      <td>10.305407</td>\n",
       "      <td>...</td>\n",
       "      <td>36.998897</td>\n",
       "      <td>217.299410</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>43.095338</td>\n",
       "      <td>213.972295</td>\n",
       "      <td>2.224474</td>\n",
       "      <td>34.276710</td>\n",
       "      <td>192.665972</td>\n",
       "      <td>16.323067</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51.019023</td>\n",
       "      <td>137.515151</td>\n",
       "      <td>100.0</td>\n",
       "      <td>43.985741</td>\n",
       "      <td>164.475186</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>31.246250</td>\n",
       "      <td>190.680080</td>\n",
       "      <td>27.072777</td>\n",
       "      <td>11.624174</td>\n",
       "      <td>...</td>\n",
       "      <td>35.411888</td>\n",
       "      <td>212.860911</td>\n",
       "      <td>5.292101</td>\n",
       "      <td>40.276011</td>\n",
       "      <td>210.683127</td>\n",
       "      <td>6.636552</td>\n",
       "      <td>27.282533</td>\n",
       "      <td>192.782329</td>\n",
       "      <td>25.991769</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.585786</td>\n",
       "      <td>135.470564</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44.739092</td>\n",
       "      <td>165.252950</td>\n",
       "      <td>98.319146</td>\n",
       "      <td>37.539161</td>\n",
       "      <td>190.283423</td>\n",
       "      <td>16.532762</td>\n",
       "      <td>10.857018</td>\n",
       "      <td>...</td>\n",
       "      <td>37.698445</td>\n",
       "      <td>210.339226</td>\n",
       "      <td>1.432499</td>\n",
       "      <td>43.350058</td>\n",
       "      <td>207.353329</td>\n",
       "      <td>2.410524</td>\n",
       "      <td>35.294313</td>\n",
       "      <td>194.218601</td>\n",
       "      <td>13.517465</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.200490</td>\n",
       "      <td>134.900501</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44.419313</td>\n",
       "      <td>166.425050</td>\n",
       "      <td>97.653799</td>\n",
       "      <td>36.961090</td>\n",
       "      <td>195.237618</td>\n",
       "      <td>17.888721</td>\n",
       "      <td>10.105051</td>\n",
       "      <td>...</td>\n",
       "      <td>40.442265</td>\n",
       "      <td>212.121656</td>\n",
       "      <td>4.315543</td>\n",
       "      <td>45.164493</td>\n",
       "      <td>209.684956</td>\n",
       "      <td>3.264676</td>\n",
       "      <td>35.703462</td>\n",
       "      <td>199.654580</td>\n",
       "      <td>14.647831</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    right hip-x  right hip-y  right hip-z  right knee-x  right knee-y  \\\n",
       "0     45.719182   132.655828        100.0     40.325270    144.253953   \n",
       "1     45.704212   132.073088        100.0     39.919103    143.095944   \n",
       "2     45.757150   133.531734        100.0     39.927445    143.484909   \n",
       "3     45.442515   134.717514        100.0     39.957581    144.659558   \n",
       "4     45.396707   133.543384        100.0     39.218107    143.900380   \n",
       "5     44.011256   135.634908        100.0     38.921592    146.247061   \n",
       "6     41.821140   135.394196        100.0     39.245708    144.679104   \n",
       "7     41.706588   132.164874        100.0     39.376977    148.178400   \n",
       "8     41.450959   135.106239        100.0     39.346177    147.794723   \n",
       "9     41.128020   136.576027        100.0     38.355909    150.163014   \n",
       "10    41.127840   136.283917        100.0     37.387879    153.316648   \n",
       "11    40.804036   137.173837        100.0     37.089713    155.274678   \n",
       "12    41.962157   139.488308        100.0     39.397172    159.122662   \n",
       "13    44.814544   137.956313        100.0     40.917605    161.034622   \n",
       "14    45.549744   137.644661        100.0     40.967553    161.810606   \n",
       "15    46.328729   139.386306        100.0     41.327667    161.808180   \n",
       "16    48.574329   140.505220        100.0     42.930678    165.276049   \n",
       "17    51.019023   137.515151        100.0     43.985741    164.475186   \n",
       "18    50.585786   135.470564        100.0     44.739092    165.252950   \n",
       "19    50.200490   134.900501        100.0     44.419313    166.425050   \n",
       "\n",
       "    right knee-z  right ankle-x  right ankle-y  right ankle-z  left hip-x  \\\n",
       "0      86.735232      34.595294     153.503292      15.948365   14.023882   \n",
       "1      94.600647      33.259860     144.370975      23.746151   13.610768   \n",
       "2      99.527298      33.742680     150.392426      28.619502   12.970735   \n",
       "3      99.949475      30.496209     155.566089      33.676301   12.325353   \n",
       "4     100.000000      33.292064     157.982605      30.920655   11.515416   \n",
       "5     100.000000      30.310831     167.000969      34.354335   10.494606   \n",
       "6     100.000000      31.755593     148.063947      36.570477    9.085493   \n",
       "7      98.631682      35.045406     133.591160      38.300539    6.035339   \n",
       "8      97.464171      36.340191     139.477843      32.979997    6.524072   \n",
       "9      94.819617      38.229298     160.915547      17.701734    6.544273   \n",
       "10     99.882976      33.484270     188.491365      24.726870    5.479543   \n",
       "11    100.000000      32.933592     195.638889      26.376265    5.150223   \n",
       "12    100.000000      35.857686     196.225231      19.031843    6.340591   \n",
       "13    100.000000      35.491095     197.132328      21.234687    7.702167   \n",
       "14    100.000000      30.285618     200.611370      27.327022    8.404307   \n",
       "15    100.000000      32.110433     196.302087      26.165533    9.204564   \n",
       "16    100.000000      37.834669     189.358980      18.526370   10.305407   \n",
       "17    100.000000      31.246250     190.680080      27.072777   11.624174   \n",
       "18     98.319146      37.539161     190.283423      16.532762   10.857018   \n",
       "19     97.653799      36.961090     195.237618      17.888721   10.105051   \n",
       "\n",
       "    ...  right toe 1-x  right toe 1-y  right toe 1-z  right toe 2-x  \\\n",
       "0   ...      35.242309     169.563125       4.569527      40.288776   \n",
       "1   ...      33.982025     164.228804      10.583161      39.392570   \n",
       "2   ...      35.799198     164.162529      16.287762      40.448978   \n",
       "3   ...      33.365414     175.780893      23.020745      37.705657   \n",
       "4   ...      37.023137     165.521434      21.643583      40.332220   \n",
       "5   ...      35.097401     181.383237      22.829052      38.631187   \n",
       "6   ...      33.757653     169.501270      24.399920      38.071680   \n",
       "7   ...      35.981628     158.822261      21.575871      40.596812   \n",
       "8   ...      39.200288     167.307636      14.538339      43.801626   \n",
       "9   ...      42.358943     183.884977       2.486237      47.355743   \n",
       "10  ...      39.542642     206.101765      10.688838      43.034060   \n",
       "11  ...      36.059611     213.511911      14.582165      40.567990   \n",
       "12  ...      37.237976     214.995300       6.612093      42.763318   \n",
       "13  ...      36.607917     219.315474       6.192468      41.517895   \n",
       "14  ...      36.031930     217.237944      12.071057      36.694410   \n",
       "15  ...      36.071895     219.783631       3.718340      41.130656   \n",
       "16  ...      36.998897     217.299410       0.000083      43.095338   \n",
       "17  ...      35.411888     212.860911       5.292101      40.276011   \n",
       "18  ...      37.698445     210.339226       1.432499      43.350058   \n",
       "19  ...      40.442265     212.121656       4.315543      45.164493   \n",
       "\n",
       "    right toe 2-y  right toe 2-z  right heel-x  right heel-y  right heel-z  \\\n",
       "0      167.144811       4.858135     33.377358    158.475119     12.045356   \n",
       "1      162.468908      11.766435     30.523745    149.269797     20.490017   \n",
       "2      161.393837      18.137520     32.021778    154.927910     24.901434   \n",
       "3      172.479690      22.709054     29.137211    159.996572     30.494509   \n",
       "4      158.163000      25.219125     33.715608    162.032827     26.092724   \n",
       "5      177.127592      23.021628     30.074487    169.837089     30.686433   \n",
       "6      165.887495      24.902200     30.416170    151.612852     33.927917   \n",
       "7      155.486118      23.801726     32.160150    134.193325     37.892515   \n",
       "8      163.451408      17.108954     32.771995    141.165296     31.850373   \n",
       "9      181.081490       4.360121     34.786343    165.851328     14.418940   \n",
       "10     203.296841      10.296150     33.042080    191.983551     20.804520   \n",
       "11     211.099269      13.447851     32.667632    197.475320     22.586241   \n",
       "12     212.886866       6.580670     33.770535    201.036066     15.520785   \n",
       "13     217.381384       6.172916     33.110456    201.036807     18.502766   \n",
       "14     216.346899      10.551490     29.971744    202.996275     24.731801   \n",
       "15     217.793346       4.825427     28.199790    199.837905     24.909436   \n",
       "16     213.972295       2.224474     34.276710    192.665972     16.323067   \n",
       "17     210.683127       6.636552     27.282533    192.782329     25.991769   \n",
       "18     207.353329       2.410524     35.294313    194.218601     13.517465   \n",
       "19     209.684956       3.264676     35.703462    199.654580     14.647831   \n",
       "\n",
       "    frame_count  \n",
       "0            39  \n",
       "1            39  \n",
       "2            39  \n",
       "3            39  \n",
       "4            39  \n",
       "5            39  \n",
       "6            39  \n",
       "7            39  \n",
       "8            39  \n",
       "9            39  \n",
       "10           39  \n",
       "11           39  \n",
       "12           39  \n",
       "13           39  \n",
       "14           39  \n",
       "15           39  \n",
       "16           39  \n",
       "17           39  \n",
       "18           39  \n",
       "19           39  \n",
       "\n",
       "[20 rows x 37 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pd.read_csv(data_path+'GVS_212_W_T2_2'+'.csv', index_col = 0)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=7, out_channels=20, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=20, out_channels=10, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        log_probs = F.log_softmax(x, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, trueY, framework, model_name):\n",
    "    '''\n",
    "    Function to evaluate ML models and plot it's confusion matrix\n",
    "    Input: model, test set, true test set labels, framework name, model name\n",
    "    Computes the stride and subject wise test set evaluation metrics \n",
    "    Returns: Prediction probabilities for HOA/MS/PD and stride and subject wise evaluation metrics \n",
    "    (Accuracy, Precision, Recall, F1 and AUC)\n",
    "    '''\n",
    "    test_labels = trueY['label'] #Dropping the PID\n",
    "#     print ('Test labels', test_labels)\n",
    "    predictions = model.predict(test_features)\n",
    "#     print ('Predictions', predictions)\n",
    "    \n",
    "    #Stride wise metrics \n",
    "    acc = accuracy_score(test_labels, predictions)\n",
    "    #For multiclass predictions, we need to use marco/micro average\n",
    "    p = precision_score(test_labels, predictions, average='macro')  \n",
    "    r = recall_score(test_labels, predictions, average = 'macro')\n",
    "    f1 = f1_score(test_labels, predictions, average= 'macro')\n",
    "    \n",
    "    try:\n",
    "        prediction_prob = model.predict_proba(test_features) #Score of the class with greater label\n",
    "#         print ('Prediction Probability', model.predict_proba(test_features))\n",
    "        \n",
    "    except:\n",
    "        prediction_prob = model.best_estimator_._predict_proba_lr(test_features) #For linear SVM\n",
    "#         print ('Prediction Probability', model.best_estimator_._predict_proba_lr(test_features))\n",
    "    \n",
    "    #For computing the AUC, we would need prediction probabilities for all the 3 classes \n",
    "    auc = roc_auc_score(test_labels, prediction_prob, multi_class = 'ovo', average= 'macro')\n",
    "    print('Stride-based model performance: ', acc, p, r, f1, auc)\n",
    "    \n",
    "    #For computing person wise metrics \n",
    "    temp = copy.deepcopy(trueY) #True label for the stride \n",
    "    temp['pred'] = predictions #Predicted label for the stride \n",
    "    #Saving the stride wise true and predicted labels for calculating the stride wise confusion matrix for each model\n",
    "    temp.to_csv(results_path+ framework + '\\\\stride_wise_predictions_' + str(model_name) + '_' + framework + '.csv')\n",
    "    \n",
    "    x = temp.groupby('PID')['pred'].value_counts().unstack()\n",
    "    #Input for subject wise AUC is probabilities at columns [0, 1, 2]\n",
    "    proportion_strides_correct = x.divide(x.sum(axis = 1), axis = 0).fillna(0) \n",
    "    proportion_strides_correct['True Label'] = trueY.groupby('PID').first()\n",
    "    #Input for precision, recall and F1 score\n",
    "    proportion_strides_correct['Predicted Label'] = proportion_strides_correct[[0, 1, 2]].idxmax(axis = 1) \n",
    "    #Saving the person wise true and predicted labels for calculating the subject wise confusion matrix for each model\n",
    "    proportion_strides_correct.to_csv(results_path+ framework + '\\\\person_wise_predictions_' + \\\n",
    "                                      str(model_name) + '_' + framework + '.csv')\n",
    "    try:\n",
    "        print (model.best_estimator_)\n",
    "    except:\n",
    "        pass\n",
    "    #Person wise metrics \n",
    "    person_acc = accuracy_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'])\n",
    "    person_p = precision_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                               average = 'macro')\n",
    "    person_r = recall_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                            average = 'macro')\n",
    "    person_f1 = f1_score(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                         average = 'macro')\n",
    "    person_auc = roc_auc_score(proportion_strides_correct['True Label'], proportion_strides_correct[[0, 1, 2]], \\\n",
    "                               multi_class = 'ovo', average= 'macro')\n",
    "    print('Person-based model performance: ', person_acc, person_p, person_r, person_f1, person_auc)\n",
    "      \n",
    "    #Plotting and saving the subject wise confusion matrix \n",
    "    plt.figure()\n",
    "    confusion_matrix = pd.crosstab(proportion_strides_correct['True Label'], proportion_strides_correct['Predicted Label'], \\\n",
    "                                   rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap=\"YlGnBu\")\n",
    "    plt.savefig(results_path + framework+'\\\\CFmatrix_task_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "    plt.show()\n",
    "    return proportion_strides_correct[[0, 1, 2]], [acc, p, r, f1, auc, person_acc, person_p, person_r, person_f1, person_auc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set ROC curves for cohort prediction \n",
    "def plot_ROC(ml_models, testY, predicted_probs_person, framework):\n",
    "    '''\n",
    "    Function to plot the ROC curve for models given in ml_models list \n",
    "    Input: ml_models (name of models to plot the ROC for),  test_Y (true test set labels with PID), \n",
    "        predicted_probs_person (predicted test set probabilities for all 3 classes - HOA/MS/PD), framework (WtoWT / VBWtoVBWT)\n",
    "    Plots and saves the ROC curve with individual class-wise plots and micro/macro average plots \n",
    "    '''\n",
    "    n_classes = 3 #HOA/MS/PD\n",
    "    cohort = ['HOA', 'MS', 'PD']\n",
    "    ml_model_names = {'random_forest': 'RF', 'adaboost': 'Adaboost', 'kernel_svm': 'RBF SVM', 'gbm': 'GBM', \\\n",
    "                      'xgboost': 'Xgboost', 'knn': 'KNN', 'decision_tree': 'DT',  'linear_svm': 'LSVM', \n",
    "                 'logistic_regression': 'LR', 'mlp': 'MLP'}\n",
    "    #PID-wise true labels \n",
    "    person_true_labels = testY.groupby('PID').first()\n",
    "    #Binarizing/getting dummies for the true labels i.e. class 1 is represented as 0, 1, 0\n",
    "    person_true_labels_binarize = pd.get_dummies(person_true_labels.values.reshape(1, -1)[0])  \n",
    "\n",
    "    sns.despine(offset=0)\n",
    "    linestyles = ['-', '-', '-', '-.', '--', '-', '--', '-', '--']\n",
    "    colors = ['b', 'magenta', 'cyan', 'g',  'red', 'violet', 'lime', 'grey', 'pink']\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for idx, ml_model in enumerate(ml_models): #Plotting the ROCs for all models in ml_models list\n",
    "        fig, axes = plt.subplots(1, 1, sharex=True, sharey = True, figsize=(6, 4.5))\n",
    "        axes.plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 3, color = 'k')\n",
    "        # person-based prediction probabilities for class 0: HOA, 1: MS, 2: PD\n",
    "        model_probs = predicted_probs_person[[ml_model+'_HOA', ml_model+'_MS', ml_model+'_PD']]\n",
    "\n",
    "        for i in range(n_classes): #For 3 classes 0, 1, 2\n",
    "            fpr[i], tpr[i], _ = roc_curve(person_true_labels_binarize.iloc[:, i], model_probs.iloc[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i]) #Computing the AUC score for each class\n",
    "            #Plotting the ROCs for the three classes separately\n",
    "            axes.plot(fpr[i], tpr[i], label = cohort[i] +' ROC (AUC = '+ str(round(roc_auc[i], 3))\n",
    "                +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[i], color = colors[i]) \n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area (AUC)\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(person_true_labels_binarize.values.ravel(), model_probs.values.ravel())\n",
    "        #Micro average AUC of ROC value\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"]) \n",
    "        #Plotting the micro average ROC \n",
    "        axes.plot(fpr[\"micro\"], tpr[\"micro\"], label= 'micro average ROC (AUC = '+ str(round(roc_auc[\"micro\"], 3))\n",
    "                +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[3], color = colors[3])\n",
    "\n",
    "        #Compute the macro-average ROC curve and AUC value\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # First aggregate all false positive rates\n",
    "        mean_tpr = np.zeros_like(all_fpr) # Then interpolate all ROC curves at this points\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes  # Finally average it and compute AUC\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        #Macro average AUC of ROC value \n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        #Plotting the macro average AUC\n",
    "        axes.plot(fpr[\"macro\"], tpr[\"macro\"], label= 'macro average ROC (AUC = '+ str(round(roc_auc[\"macro\"], 3))\n",
    "            +')', linewidth = 3, alpha = 0.8, linestyle = linestyles[4], color = colors[4])\n",
    "\n",
    "        axes.set_ylabel('True Positive Rate')\n",
    "        axes.set_title('Task generalization '+framework + ' '+ ml_model_names[ml_model])\n",
    "        plt.legend()\n",
    "        # axes[1].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "\n",
    "        axes.set_xlabel('False Positive Rate')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_path + framework+'\\\\ROC_task_generalize_' + framework + '_'+ ml_model+ '.png', dpi = 350)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\'\n",
    "data_path = path+'downsampled_strides\\\\'\n",
    "labels_file = path+ 'labels.csv'\n",
    "results_path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\DLresults\\\\CNN1D\\\\'\n",
    "\n",
    "use_cuda = torch.cuda.is_available() #use_cuda is True if cuda is available \n",
    "set_random_seed(0, use_cuda) #Setting a fixed random seed for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.random.uniform(-10, 10, 70).reshape(1, 7, -1)\n",
    "# Y = np.random.randint(0, 9, 10).reshape(1, 1, -1)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 3\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Error messages are much better if on CPU\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, preds) in enumerate(trainDataloader):\n",
    "        #Get data to cuda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        #print(\"data nan? \", (torch.isnan(data)).any())\n",
    "        #print(data)\n",
    "        #print(\"original data shape: \",data.shape)\n",
    "        label = preds.to(device=device)\n",
    "        #print(label)\n",
    "        #print(\"original label shape: \",label.shape)\n",
    "\n",
    "        # forward\n",
    "        #print(targets)\n",
    "        pred = model(data.float())\n",
    "        pred = torch.squeeze(pred)\n",
    "        #print(\"post squeeze output shape: \", pred.shape)\n",
    "        loss = criterion(pred, label.float())\n",
    "        \n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    #if loader.dataset.train:\n",
    "    #    print(\"Checking accuracy on training data\")\n",
    "    #else:\n",
    "    #    print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x.float())\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Got {num_correct} / {num_samples} with \\\n",
    "              accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n",
    "        )\n",
    "    # Set model back to train\n",
    "    model.train()\n",
    "\n",
    "model = CNN1D().double()\n",
    "print(model(torch.tensor(X)).shape)\n",
    "torch.save(model.state_dict(), results_path+framework+'\\\\'+ model_save_name+ '.pt')\n",
    "# model = cnn_model.ConvNet(32).double()\n",
    "# model.cuda()\n",
    "# model.load_state_dict(torch.load(results_path+framework+'\\\\'+ model_save_name+ '.pt'))\n",
    "\n",
    "#Maybe use skorch for hyperparameter grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of subjects in training and test sets: 32 26\n",
      "Subjects in test set, which are not in training set\n",
      "[403]\n",
      "Subjects in training set, which are not in test set\n",
      "[312, 102, 112, 113, 115, 123, 124]\n",
      "Number of subjects in training and test sets after reduction: 25 25\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv(labels_file)\n",
    "#Trial W for training \n",
    "trialW = labels[labels['scenario']=='W']\n",
    "#Trial WT for testing \n",
    "trialWT = labels[labels['scenario']=='WT']\n",
    "#Returning the PIDs of common subjects in training and testing set\n",
    "pids_retain_trialW, pids_retain_trialWT = list_subjects_common_across_train_test(trialW, trialWT)\n",
    "            \n",
    "# cols_to_drop = ['PID', 'key', 'cohort', 'trial', 'scenario', 'video', 'stride_number', 'label']\n",
    "# #Shuffling the training stride data\n",
    "# trialW_reduced = shuffle(trialW_reduced, random_state = 0)\n",
    "# trainX = trialW_reduced.drop(cols_to_drop, axis = 1)\n",
    "# trainY = trialW_reduced[['PID', 'label']]\n",
    "# print ('Training shape', trainX.shape, trainY.shape)\n",
    "\n",
    "# #Shuffling the testing stride data \n",
    "# trialWT_reduced = shuffle(trialWT_reduced, random_state = 0)\n",
    "# testX = trialWT_reduced.drop(cols_to_drop, axis = 1)\n",
    "# testY = trialWT_reduced[['PID', 'label']] #PID to compute person based metrics later \n",
    "# print ('Testing shape', testX.shape, testY.shape)\n",
    "\n",
    "# #Normalize according to z-score standardization\n",
    "# norm_mean, norm_sd = normalize(trainX, 'z')\n",
    "# trainX_norm = (trainX-norm_mean)/norm_sd\n",
    "# testX_norm = (testX-norm_mean)/norm_sd\n",
    "\n",
    "# #Total strides and imbalance of labels in the training and testing set\n",
    "# #Training set \n",
    "# print('Strides in training set: ', len(trialW_reduced))\n",
    "# print ('HOA, MS and PD strides in training set:\\n', trialW_reduced['cohort'].value_counts())\n",
    "\n",
    "# #Test Set\n",
    "# print('\\nStrides in test set: ', len(trialWT_reduced)) \n",
    "# print ('HOA, MS and PD strides in test set:\\n', trialWT_reduced['cohort'].value_counts())\n",
    "# print ('Imbalance ratio (controls:MS:PD)= 1:X:Y\\n', trialWT_reduced['cohort'].value_counts()/trialWT_reduced['cohort'].value_counts()['HOA'])\n",
    "\n",
    "# framework = 'WtoWT' #Defining the task generalization framework of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['CNN1D']\n",
    "metrics = pd.DataFrame(columns = ml_models) #Dataframe to store accuracies for each ML model for raw data \n",
    "\n",
    "#For storing predicted probabilities for person (for all classes HOA/MS/PD) to show ROC curves \n",
    "predicted_probs_person = pd.DataFrame(columns = [ml_model + cohort for ml_model in ml_models for cohort in ['_HOA', '_MS', '_PD'] ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ml_model in ml_models:\n",
    "    print (ml_model)\n",
    "    predict_probs_person, stride_person_metrics = models(trainX_norm, trainY, testX_norm, testY, ml_model, framework)  \n",
    "    metrics[ml_model] = stride_person_metrics\n",
    "    predicted_probs_person[ml_model+'_HOA'] = predict_probs_person[0]\n",
    "    predicted_probs_person[ml_model+'_MS'] = predict_probs_person[1]\n",
    "    predicted_probs_person[ml_model+'_PD'] = predict_probs_person[2]\n",
    "    print ('********************************')\n",
    "\n",
    "metrics.index = ['stride_accuracy', 'stride_precision', 'stride_recall', 'stride_F1', 'stride_AUC', 'person_accuracy', \n",
    "                     'person_precision', 'person_recall', 'person_F1', 'person_AUC']  \n",
    "metrics.to_csv(results_path+'task_generalize_'+framework+'_result_metrics.csv')\n",
    "predicted_probs_person.to_csv(results_path+'task_generalize_'+framework+'_prediction_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(ml_models, testY, predicted_probs_person, framework)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
