{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gait Video Study \n",
    "### Validating the estimates 3D poses via CoP computed on the treadmill \n",
    "### This code makes (and saves as csv) the dataframe for each video containing their relevant frames, corresponding feet coordinates for these frames and their relative treadmill extracted CoP values \n",
    "\n",
    "We will do qualitative and quantitative validation for CoP. \n",
    "* First, we need to align the treadmill GaitCycles.csv file to the video time. This will help us align the corresponding video frames to the gait events i.e. HSR/HSL/TOR and TOL to be specific. \n",
    "* Once we know for each video, which frame numbers correspond to heel strikes and toe offs, we compute the sequence of frames that are in single support left phase, frames that are in single support right phase and similarly in double support phase. Thus, each frame of a video is labelled to be in SSR, SSL or DS phase. \n",
    "* Now, for each frame in DS phase, use the computed real world x, y coordinates for big toe, small toe and heel to make 2 triangular regions for both left and right feet, since in this phase, both feet are on ground and thus impact the center of pressure. Now, plot the corresponding actual COPX, COPY coordinate (as a red dot) for this particular frame. If this red dot lies in the shaded region of computed CoP drawn, we are good to claim that actual CoP lies in the approximate computed CoP region. Similarly, for each frame in the SSL phase, since left foot rests on ground for this phase, the CoP must be determined using the left foot, and hence use the computed x, y coordinates of the left big toe, small toe and heel to draw a shaded triangular region spanned by CoP for this frame, and draw the red dot for the actual x, y of CoP for this frame and if it lies within the shaded region, we are good to claim that actual and computed CoP region match. Now, for each frame in SSR phase, the shaded CoP region must be determined using the right feet's big/small toe and heel's x, y coordinates and if the actual CoP's x, y is bounded in this shaded computed CoP's region, we are good. \n",
    "\n",
    "* For qualitative validation, we plot these above mentioned regions for computed CoP and actual CoPs as markers in/out of that region for each video. If we do this for a complete stride, it should follow a butterfly pattern. And hence the inverted triangles and hexagons should occur in a butterfly pattern.\n",
    "* For quantitative validation, we will call it success (1) if the actual CoP is bounded by the computed CoP shaded region and failure (0) otherwise for every frame of every video for every trial and cohort. \n",
    "* Further, for more precise quantitative validation, we can find the lateral, anterior-posterior and euclidean distance of the actual (COP_x, COP_y) with the centroid of our region drawn. This gives us a numerical value quantifying the error in the true and predicted CoP. This step can especially be done for only wrongly predicted values, to further check what is the measure of wrongly predicted values. Further, we may check that we may have error most in the lateral direction or most in the AP direction or eucliean only. \n",
    "* Based on the statistics of these success and failure counts, we can quantify the performance of our marker estimation framework using CoP validation. \n",
    "* Further, we can try to correlate/have a look at the distribution pattern to relate the correctness of CoP (either quantified using binary scores or using the numerical scores) with the confidence scores predicted by the OpenPose algorithm. Now, since we are only using toes and heel coordinates to draw the CoM trajectory/region, we should only use the confidence scores for heel and toes for this correlation. To be precise, we can average the confidence scores of left feet's heel and 2 toes to get the aggregated confidence score for frames in left single support, similarly, we can average the confidence scores of right feet's heel and 2 toes to get the aggregated confidence score for frames in right single support, and average the confidence scores of both the left and right feet's heel and toes to get the aggregated confidence scores for frames in double support. Now this correlation/relation between the confidence score for each frame and it's coorsponding correctness of CoP metric can be either done on a frame by frame basis. Or rather we can aggreagte all frames over a stride and do the relationship analysis on a stride by stride basis based on some aggregatd confidence scores of stride with some aggregated correctness of CoP score over each stride. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import imports \n",
    "reload(imports)\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_file = pd.read_csv('C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\GaitVideoData\\\\video\\\\labels.csv', index_col = 0)\n",
    "# pd.DataFrame(labels_file.video.unique(), columns = ['video']).to_csv(cop_path+'treamill_video_cop_sync.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder for CoP validation\n",
    "cop_path = 'C:\\\\Users\\\\Rachneet Kaur\\\\Box\\\\Gait Video Project\\\\CoPvalidation\\\\'\n",
    "#Subfolder containing the sync files between frame numbers and treadmill identified events for each video\n",
    "treadmill_video_sync_files = cop_path + 'CoP_treadmill_video_sync\\\\'\n",
    "#Path to store the new dataframes to be created for CoP validation \n",
    "path_viz_dataframes = cop_path + 'CoP_dataframes_for_viz\\\\'\n",
    "#Path to log file corresponding to the sync files between frame numbers and treadmill identified events for each video\n",
    "sync_log_file = cop_path + 'treamill_video_cop_sync.csv'\n",
    "\n",
    "#Path for reading the frame coordinates and OpenPose confidence scores from (toes and heel in particular)\n",
    "frame_path = cop_path + '..\\\\GaitVideoData\\\\video\\\\multi_view_merged_data\\\\' \n",
    "#Path for the RAWDATA.csv containing the COP values extracted by the treadmill wrt the time of the walking trial \n",
    "cop_treadmill_path = cop_path + '..\\\\GaitCSVData\\\\csv\\\\'\n",
    "\n",
    "#Configuration for which to run the code for \n",
    "cohorts = ['\\\\HOA', '\\\\MS', '\\\\PD', '\\\\ExtraHOA']\n",
    "trials = ['\\\\beam_walking', '\\\\walking']\n",
    "\n",
    "# for every GaitCycle file, a sequence of walk will always start with a heel strike on the right foot.\n",
    "# Thus the order of the Gait event points would be HSR, TOL, MidSSR, HSL, TOR and MidSSL.\n",
    "gait_type = np.array(['HSR', 'TOL', 'MidSSR', 'HSL', 'TOR', 'MidSSL'])\n",
    "\n",
    "trial_dict = {'BW': 'beam_walking', 'W': 'walking'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Need to run only once to create the new sync file \n",
    "# #Reading the log of the treadmill and video syncs \n",
    "# sync_log = pd.read_csv(sync_log_file, index_col = 0)\n",
    "# #Setting the new scenario column for marking the video as belonging to one of the W/WT/VBW/VBWT trials \n",
    "# sync_log.set_index('video', inplace=True)\n",
    "# sync_log['scenario'] = labels_file.groupby('video').first()['scenario']\n",
    "# sync_log.reset_index(inplace=True)\n",
    "# print ('Total video files: ', sync_log.shape[0])\n",
    "# #Saving the new sync file with scenario marked \n",
    "# sync_log.to_csv(sync_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid strides in the gait_cycles.csv file \n",
    "def get_cycle(dataframe):\n",
    "    stride_start = min(dataframe.loc[dataframe.EventType == 'HSR'].index)\n",
    "    stride_end = max(dataframe.loc[dataframe.EventType == 'MidSSL'].index)   \n",
    "    return dataframe.loc[stride_start:stride_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the indexing for the cropped dataframe \n",
    "def change_index(dataframe):\n",
    "    dataframe.index = range(len(dataframe))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the valid index in order: HSR-TOL-MidSSR-HSL-TOR-MidSSL\n",
    "def set_complete(data_frame):\n",
    "    # input is the Dataframe includes ONLY valid points \n",
    "    # get all the index of HSR since it starts with heal strike left\n",
    "    # if the length of last gait cycle contain HSR does not equals to 6, then ignore it\n",
    "    \n",
    "    HSR = data_frame.loc[data_frame.EventType == 'HSR'].index\n",
    "    last_idx = HSR[-1]\n",
    "    last_all_idx = data_frame.index[-1]\n",
    "    # if the last gait cycles contains HSR is not a valid gait cycle, then we should consider the last second HSR instead.\n",
    "    if((last_all_idx-last_idx) < 5):\n",
    "        HSR = HSR[0:-1] \n",
    "    else:\n",
    "        HSR = HSR\n",
    "    \n",
    "    # get all the valid index in order: HSR-TOL-MidSSR-HSL-TOR-MidSSL\n",
    "    valid = []\n",
    "    for idx_HSR in HSR:\n",
    "        if (((idx_HSR + 1) in data_frame.index) & ((idx_HSR + 2) in data_frame.index) &\n",
    "            ((idx_HSR + 3) in data_frame.index) & ((idx_HSR + 4) in data_frame.index) & \n",
    "            ((idx_HSR + 5) in data_frame.index)):\n",
    "            # the valid index exist in the dataframe.\n",
    "            if((data_frame.loc[idx_HSR + 1].EventType == 'TOL') & (data_frame.loc[idx_HSR + 2].EventType == 'MidSSR') & \n",
    "               (data_frame.loc[idx_HSR + 3].EventType == 'HSL') & (data_frame.loc[idx_HSR + 4].EventType == 'TOR') & \n",
    "               (data_frame.loc[idx_HSR + 5].EventType == 'MidSSL')):\n",
    "                valid.extend(range(idx_HSR, idx_HSR+6))\n",
    "    #returns the list of valid indices which form complete strides \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the files to delete missing and invalid data \n",
    "def cleaning(gaitcycles_dataframe):         \n",
    "    #Reducing to complete strides data \n",
    "    #Making sure we start at the HSR and end at the MidSSL\n",
    "    gaitcycles_dataframe = get_cycle(gaitcycles_dataframe)\n",
    "    #Retaining only complete six even strides \n",
    "    indices_complete = set_complete(gaitcycles_dataframe)\n",
    "    gaitcycles_dataframe = gaitcycles_dataframe.loc[indices_complete]\n",
    "\n",
    "    #Resetting the index \n",
    "    gaitcycles_dataframe = change_index(gaitcycles_dataframe)\n",
    "    #Returning indices to identify consequetive and non-consequetive strides \n",
    "    return indices_complete, gaitcycles_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sync_files(video):\n",
    "    #Read the file for the current video syncing event types to frame numbers \n",
    "    video_csv = pd.read_excel(treadmill_video_sync_files + video + '.xlsx')\n",
    "    #Retaining only time for treadmill's CoP matching, event type for SS/DS group assignment \n",
    "    #and frame number for extracting body coordinates\n",
    "    video_csv = video_csv[['Time', 'EventType', 'frame_number']]\n",
    "    #Dropping the entries/events that could not be synced to their corresponding video frames \n",
    "    video_csv.dropna(inplace = True)\n",
    "    #Retaining the indices and corresponding rows of dataframe for 'complete' 6 event strides only\n",
    "    #Indices will help identify consequetive and non-consequetive strides \n",
    "    indices_retain, video_csv_retain = cleaning(video_csv)\n",
    "    #Converting frame number to ints \n",
    "    video_csv_retain['frame_number'] = video_csv_retain['frame_number'].astype(int)\n",
    "#     display(video_csv_retain.head(), video_csv.shape, video_csv_retain.shape)\n",
    "    return indices_retain, video_csv_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_body_coordinates(viz_df, coordinate_path, coords_of_interest, coordinate_cols):\n",
    "    '''\n",
    "    Filling up the toe and heel coordinates in cm and confidence scores in the viz dataframe for the current video\n",
    "    '''\n",
    "    #Iterating through each frame number to read it's corresponding file for body coordinates and filling them up in the dataframe\n",
    "    for frame_number in viz_df.index:\n",
    "        try: #Since some frames from the video data may be missing \n",
    "            frame = pd.read_csv(coordinate_path+str(frame_number)+'.csv', index_col = 0)\n",
    "            #display(frame)\n",
    "            #For each frame, we are interested in only feet coordinates for drawing the CoM area\n",
    "            #Further, we use only x, y coordinates and confidence scores for this validation analysis \n",
    "            frame_coords_of_interest = frame.loc[coords_of_interest][['x', 'y', 'confidence']] \n",
    "            #Filling up the feet coordinates for a particular frame \n",
    "            viz_df.loc[frame_number, coordinate_cols] = frame_coords_of_interest.values.flatten()\n",
    "        except: \n",
    "            #If the particular frame was missing in video data, let the values be NaN for that missing video data frame  \n",
    "            pass\n",
    "#     display(viz_df.head())\n",
    "    return viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_treadmill_COP_values(viz_df, cop_path):\n",
    "    '''\n",
    "    Filling up the treadmill CoP values \n",
    "    '''\n",
    "    #For each trial/video, reading the corresponding RAWDATA.csv file containing the CoP_x, CoP_y values spaced at 0.002 seconds \n",
    "    cop_file = pd.read_csv(cop_path, header = 1)\n",
    "    #Retaining only time, COPX, COPY columns from the file\n",
    "    cop_file = cop_file[['Time', 'COPX', 'COPY']]\n",
    "    #     display(cop_file.head())\n",
    "\n",
    "    #Since the frame times are different than the 0.002 spacing of time in the RAWDATA.csv file, we find the \n",
    "    #closest time from the RAWDATA.csv file (since it's much more granular) to each frame's time in viz_df\n",
    "    #We use the fact that time is sorted in increasing order in the RAWDATA.csv file\n",
    "    cop_closest_times_left_bound = [cop_file['Time'][cop_file['Time']>(viz_df['Time'].iloc[i]-(1/60))].iloc[0] for i in range(len(viz_df))]\n",
    "    cop_closest_times_right_bound = [cop_file['Time'][cop_file['Time']<(viz_df['Time'].iloc[i]+(1/60))].iloc[-1] for i in range(len(viz_df))]\n",
    "    cop_file.set_index('Time', inplace = True)\n",
    "    #Assinging the COPX and COPY corresponding to the closest times in the RAWDATA.csv file to frame times \n",
    "    treadmill_COP_x = [cop_file.loc[i:j]['COPX'].mean() for i, j in zip(cop_closest_times_left_bound, cop_closest_times_right_bound)]\n",
    "    treadmill_COP_y = [cop_file.loc[i:j]['COPY'].mean() for i, j in zip(cop_closest_times_left_bound, cop_closest_times_right_bound)]\n",
    "    return treadmill_COP_x, treadmill_COP_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_up_support_types(viz_df):\n",
    "    '''\n",
    "    Filling up the support types\n",
    "    '''\n",
    "    #Marking the frame numbers of frames for HSR, TOL, HSL and TOR events \n",
    "    HSR_frames = viz_df[viz_df['EventType'] == 'HSR'].index\n",
    "    TOL_frames = viz_df[viz_df['EventType'] == 'TOL'].index\n",
    "    HSL_frames = viz_df[viz_df['EventType'] == 'HSL'].index\n",
    "    TOR_frames = viz_df[viz_df['EventType'] == 'TOR'].index\n",
    "\n",
    "    '''For our complete strides with sequence of events being: HSR-TOL-MidSSR-HSL-TOR-MidSSL-next stride's HSR, we compute the \n",
    "    initial double support as frames between (including) HSR and (not including) TOL, right single support as frames between (including) TOL \n",
    "    and (not including) HSL, terminal double support as frames between (including) HSL and (not including) TOR, left single support as \n",
    "    frame between (including) TOR and (not including) HSR. \n",
    "    The reason for including the left interval but not the right one, is because these events are typically treated as the \n",
    "    boundaries of the different states, so in the case of double support: HSR_time < time < TOL_time. \n",
    "    For single support, you can carry out the same process, where: TOL < time < HSL_time is right single support. \n",
    "    Now since when we compute the frame number, the frame no.s we assigned were one integer ahead round of float frame number \n",
    "    we got for each event. So according to HSR_time < time < TOL_time rule, if frame 231 is TOL_frame, then it should not belong to \n",
    "    double support but belong to right SS. \n",
    "    Hence for our case, double support frames are HSR_frame<=frames<TOL_frame, the right single support frames are TOL_frame<=frames<HSL_frame\n",
    "    and so on.\n",
    "    '''\n",
    "    #Initial double support\n",
    "    initial_double_support_indices = [viz_df.loc[HSR_frames[i]:TOL_frames[i]-1].index.values for i in range(len(HSR_frames))]\n",
    "    #-1 from the right limit of the interval to make sure we do not include frame of TOL to the initial double support frames list\n",
    "    initial_double_support_list = np.concatenate(initial_double_support_indices).ravel().tolist()\n",
    "\n",
    "    #Right single support \n",
    "    right_single_support_indices = [viz_df.loc[TOL_frames[i]:HSL_frames[i]-1].index.values for i in range(len(TOL_frames))]\n",
    "    #-1 from the right limit of the interval to make sure we do not include frame of HSL to the right single support frames list\n",
    "    right_single_support_list = np.concatenate(right_single_support_indices).ravel().tolist()\n",
    "\n",
    "    #Terminal double support\n",
    "    terminal_double_support_indices = [viz_df.loc[HSL_frames[i]:TOR_frames[i]-1].index.values for i in range(len(HSL_frames))]\n",
    "    terminal_double_support_list = np.concatenate(terminal_double_support_indices).ravel().tolist()\n",
    "\n",
    "    #Left single support\n",
    "    left_single_support_indices = [viz_df.loc[TOR_frames[i]:HSR_frames[i+1]-1].index.values for i in range(len(HSR_frames)-1)]\n",
    "    left_single_support_list = np.concatenate(left_single_support_indices).ravel().tolist()\n",
    "\n",
    "    return initial_double_support_indices, initial_double_support_list, right_single_support_indices, right_single_support_list, \\\n",
    "            terminal_double_support_indices, terminal_double_support_list, left_single_support_indices, left_single_support_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_consequetive_strides(indices_retain, video_csv_retain, viz_df):\n",
    "    #Handling non-consequetive strides \n",
    "    #Using the indices for the complete strides to infer inconsequtive strides' MidSSL frame number\n",
    "    non_consequetive_stride_MidSSLs = np.where(np.array(list(map(operator.sub, indices_retain[1:], indices_retain[:-1])))!=1)[0]\n",
    "    print ('No. of non-consequetive strides: ', len(non_consequetive_stride_MidSSLs))\n",
    "    #Looping through each non consequetive strides' MidSSL\n",
    "    for non_consequetive_stride_MidSSL in non_consequetive_stride_MidSSLs:\n",
    "        #Getting the frame nunber for TOR and HSR (since for non-consequetive strides, left single support from \n",
    "        #current stride's TOR-next strides' HSR is invalid!)\n",
    "        non_consequetive_stride_TOR = video_csv_retain.iloc[non_consequetive_stride_MidSSL-1].frame_number\n",
    "        non_consequetive_stride_HSR = video_csv_retain.iloc[non_consequetive_stride_MidSSL+1].frame_number\n",
    "        #Changing all the frames between TOR (including) and HSR (not including) support time to 'non_consequetive_stride' keyword\n",
    "        viz_df.loc[non_consequetive_stride_TOR:non_consequetive_stride_HSR-1]['support_type'] = 'non_consequetive_stride'\n",
    "    return viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_grouped_by_support_type(indices, coordinate_cols, viz_df):\n",
    "    #Starting frame numbers for initial double support \n",
    "    frame_start = [index_list[0] for index_list in indices]\n",
    "    #Ending frame numbers for initial double support\n",
    "    frame_end = [index_list[-1] for index_list in indices]\n",
    "    support_type = [viz_df.loc[index_list[0]].support_type for index_list in indices]\n",
    "    coordinate_cols_mean = [viz_df[coordinate_cols].loc[index_list].mean().values for index_list in indices]\n",
    "    treadmill_cop_mean = [viz_df[['treadmill_COP_x',  'treadmill_COP_y']].loc[index_list].mean().values for index_list in indices]\n",
    "\n",
    "    time_start = viz_df['Time'].loc[frame_start].values\n",
    "    time_end = viz_df['Time'].loc[frame_end].values\n",
    "\n",
    "    x = [sum(list([ [time_start[i]], [time_end[i]], [frame_start[i]], [frame_end[i]], [support_type[i]], list(coordinate_cols_mean[i]), list(treadmill_cop_mean[i])]), []) \\\n",
    "         for i in range(len(frame_start))]\n",
    "    return pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frame_and_support_group_dataframes(index, viz_df_column_names):\n",
    "    video = index['video']\n",
    "    cohort = index['cohort']\n",
    "    trial = trial_dict[index['trial']]\n",
    "    \n",
    "    #For each video, we will create a dataframe and a csv file corresponding to frame coordinates and treadmill CoP\n",
    "    viz_df = pd.DataFrame(columns = viz_df_column_names)\n",
    "    #Retaining the complete strides only from the synced treadmill six event times and video frames \n",
    "    indices_retain, video_csv_retain = clean_sync_files(video)\n",
    "    \n",
    "    #Filling up the event type, frame number and time columns\n",
    "    viz_df[['Time', 'EventType', 'frame_number']] = copy.deepcopy(video_csv_retain)\n",
    "    viz_df.set_index('frame_number', inplace=True)\n",
    "    #Listing all the frame numbers from the first HSR to the last MidSSL for viz_df \n",
    "    display(viz_df.head(), viz_df.shape)\n",
    "    print (np.arange(min(viz_df.index), max(viz_df.index)+1))\n",
    "    viz_df = viz_df.reindex(np.arange(min(viz_df.index), max(viz_df.index)+1))\n",
    "    display(viz_df.head())\n",
    "    #Filling up the time using interpolation (this will indeed follow that each frame is 1/30 seconds apart since our FPS=30)\n",
    "    viz_df.interpolate(method = 'index', inplace= True)\n",
    "    \n",
    "    #Filling up the toe and heel coordinates in cm and confidence scores in the viz dataframe for the current video\n",
    "    #To fill up the feet coordinates for each video, setting the path for body coordinate files \n",
    "    try: \n",
    "        coordinate_path = frame_path+ cohort + '\\\\' + trial + '\\\\' + video + '\\\\hip_height_normalized\\\\'\n",
    "    except: #For ExtraHOA files \n",
    "        coordinate_path = frame_path+ 'ExtraHOA' + '\\\\' + trial + '\\\\' + video + '\\\\hip_height_normalized\\\\'\n",
    "    viz_df = fill_up_body_coordinates(viz_df, coordinate_path, coords_of_interest, coordinate_cols)\n",
    "\n",
    "    #Filling up the treadmill CoP values \n",
    "    #Note that the video extracted coordinates are in 'cm', but the treadmill's CoP are in 'm'\n",
    "    cop_path = cop_treadmill_path + cohort + '\\\\' + trial + '\\\\' + video  + '_RAWDATA.csv'\n",
    "    treadmill_COP_x, treadmill_COP_y = fill_up_treadmill_COP_values(viz_df, cop_path) \n",
    "    viz_df['treadmill_COP_x'] = treadmill_COP_x\n",
    "    viz_df['treadmill_COP_y'] = treadmill_COP_y\n",
    "        \n",
    "    #Filling up the support types \n",
    "    initial_double_support_indices, initial_double_support_list, right_single_support_indices, right_single_support_list, \\\n",
    "    terminal_double_support_indices, terminal_double_support_list, left_single_support_indices, left_single_support_list = fill_up_support_types(viz_df)\n",
    "\n",
    "    #For frame numbers corresponding to each support group, assigning the relative label to 'support_type' column in the viz dataframe\n",
    "    viz_df.loc[initial_double_support_list, 'support_type'] = 'initial DS'\n",
    "    viz_df.loc[right_single_support_list, 'support_type'] = 'right SS'\n",
    "    viz_df.loc[terminal_double_support_list, 'support_type'] = 'terminal DS'\n",
    "    viz_df.loc[left_single_support_list, 'support_type'] = 'left SS'\n",
    "    \n",
    "    #Since body coordinates are recorded in 'cm', but treadmill CoP in 'm', converting CoP values to 'cm'\n",
    "    viz_df['treadmill_COP_x'] = 100*viz_df['treadmill_COP_x']\n",
    "    viz_df['treadmill_COP_y'] = 100*viz_df['treadmill_COP_y']\n",
    "    \n",
    "    #Handling non consequetive strides \n",
    "    viz_df = handle_non_consequetive_strides(indices_retain, video_csv_retain, viz_df)\n",
    "    \n",
    "    #Saving 2 dataframes, one being frame wise for each video and other being group wise where we have 4 groups, namely initial/terminal\n",
    "    #double support and left/right single support per stride of the video \n",
    "    #Ideally, we should use the frame wise computed coordinates and CoPs for vizualization purposes and support group wise \n",
    "    #(basically, average all the frames per support group) computed coordinates and CoPs for validation purposes. \n",
    "    '''\n",
    "    You need the average COP and frame coordinates because you are trying to minimize noise. \n",
    "    You are trying to average across the few frames within each event to get an estimate of where the average COP position is.\n",
    "    '''\n",
    "    #New dataframe which contains coordinates and treadmill CoPs grouped by support type, i.e. each stride of the video has only 4 entries,\n",
    "    #for left/right SS and initial/terminal DS\n",
    "    #Columns for the new reduced dataframe are: frame_number_start, frame_number_end, time_start, time_end, support type, average of all\n",
    "    #coordinates, confidences and treadmill CoP coordinates\n",
    "\n",
    "    #Initial double support \n",
    "    initial_double_support_grouped_data = generate_data_grouped_by_support_type(initial_double_support_indices, coordinate_cols, viz_df)\n",
    "    #Right single support \n",
    "    right_single_support_grouped_data = generate_data_grouped_by_support_type(right_single_support_indices, coordinate_cols, viz_df)\n",
    "    #Terminal double support \n",
    "    terminal_double_support_grouped_data = generate_data_grouped_by_support_type(terminal_double_support_indices, coordinate_cols, viz_df)\n",
    "    #Left single support \n",
    "    left_single_support_grouped_data = generate_data_grouped_by_support_type(left_single_support_indices, coordinate_cols, viz_df)\n",
    "\n",
    "    #Concatenating all the four support type groups \n",
    "    viz_df_grouped_by_support_type = pd.concat((initial_double_support_grouped_data, right_single_support_grouped_data, terminal_double_support_grouped_data, \\\n",
    "               left_single_support_grouped_data), ignore_index=True)\n",
    "    viz_df_grouped_by_support_type.columns = viz_df_grouped_by_support_type_column_names\n",
    "    \n",
    "    #Sorting by starting frame number to arrange in correct strides order \n",
    "    viz_df_grouped_by_support_type = viz_df_grouped_by_support_type.sort_values(by = 'frame_number_start')\n",
    "    viz_df_grouped_by_support_type.reset_index(inplace = True)\n",
    "    viz_df_grouped_by_support_type.drop('index', axis = 1, inplace = True)\n",
    "    \n",
    "    viz_df.reset_index(inplace = True)\n",
    "    \n",
    "    #Saving both the frame wise and support group wise dataframes to .csvs\n",
    "    viz_df.to_csv(path_viz_dataframes+video+'.csv')\n",
    "    viz_df_grouped_by_support_type.to_csv(path_viz_dataframes+video+'_grouped_by_support_type.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total video files:  107\n",
      "Total video files for which treadmill sync exists (sync time was present in the logs and treadmill software identified valid events):  102\n"
     ]
    }
   ],
   "source": [
    "sync_log = pd.read_csv(sync_log_file, index_col = 0)\n",
    "print ('Total video files: ', sync_log.shape[0])\n",
    "\n",
    "#Reducing only to video files for which treadmill sync exists \n",
    "#(sync time was present in the logs and treadmill software identified valid events)\n",
    "sync_log = sync_log[sync_log['Sync']=='Exists']\n",
    "print ('Total video files for which treadmill sync exists (sync time was present in the logs and treadmill software identified valid events): '\\\n",
    "       , sync_log.shape[0])\n",
    "\n",
    "'''\n",
    "Columns for the vizualization dataframe for each video \n",
    "Support type is left single support, right single support, double support or NaN for the left single support when \n",
    "the strides are not consequetive \n",
    "'HSR' - 'TOL': Initial double support \n",
    "'TOL' - MidSSR' - 'HSL': Right single support\n",
    "'HSL' - TOR': Terminal double support\n",
    "'TOR' - MidSSL' - Next 'HSR': Left single support (only when the strides are consequetive, else NaN)\n",
    "Extracted (x, y) for real world 3D coordinates and OpenPose confidence scores for the feet\n",
    "Treadmill's COP_x, COP_y for validation \n",
    "'''\n",
    "#We are only extracting feet coordinates to draw center of mass trajectory from the body coordinate files\n",
    "coords_of_interest = ['left toe 1', 'left toe 2', 'left heel', 'right toe 1', 'right toe 2', 'right heel']\n",
    "\n",
    "coordinate_cols = ['left toe 1-x', 'left toe 1-y', 'left toe 1-conf', 'left toe 2-x', \\\n",
    "                      'left toe 2-y', 'left toe 2-conf', 'left heel-x', 'left heel-y', 'left heel-conf', 'right toe 1-x', 'right toe 1-y',\\\n",
    "                       'right toe 1-conf', 'right toe 2-x', 'right toe 2-y', 'right toe 2-conf', 'right heel-x', 'right heel-y', \\\n",
    "                       'right heel-conf']\n",
    "viz_df_column_names = ['Time', 'EventType', 'frame_number', 'support_type'] + coordinate_cols + ['treadmill_COP_x',  'treadmill_COP_y']\n",
    "\n",
    "#Column names for dataframe grouped by support types \n",
    "viz_df_grouped_by_support_type_column_names = ['Time_start', 'Time_end', 'frame_number_start', 'frame_number_end', 'support_type'] + coordinate_cols + ['treadmill_COP_x',  'treadmill_COP_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GVS_404_W_T1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>EventType</th>\n",
       "      <th>support_type</th>\n",
       "      <th>left toe 1-x</th>\n",
       "      <th>left toe 1-y</th>\n",
       "      <th>left toe 1-conf</th>\n",
       "      <th>left toe 2-x</th>\n",
       "      <th>left toe 2-y</th>\n",
       "      <th>left toe 2-conf</th>\n",
       "      <th>left heel-x</th>\n",
       "      <th>...</th>\n",
       "      <th>right toe 1-y</th>\n",
       "      <th>right toe 1-conf</th>\n",
       "      <th>right toe 2-x</th>\n",
       "      <th>right toe 2-y</th>\n",
       "      <th>right toe 2-conf</th>\n",
       "      <th>right heel-x</th>\n",
       "      <th>right heel-y</th>\n",
       "      <th>right heel-conf</th>\n",
       "      <th>treadmill_COP_x</th>\n",
       "      <th>treadmill_COP_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frame_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.00027</td>\n",
       "      <td>HSR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.27627</td>\n",
       "      <td>TOL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.38027</td>\n",
       "      <td>MidSSR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.48427</td>\n",
       "      <td>HSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.77827</td>\n",
       "      <td>TOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Time EventType support_type left toe 1-x left toe 1-y  \\\n",
       "frame_number                                                             \n",
       "1             9.00027       HSR          NaN          NaN          NaN   \n",
       "9             9.27627       TOL          NaN          NaN          NaN   \n",
       "12            9.38027    MidSSR          NaN          NaN          NaN   \n",
       "15            9.48427       HSL          NaN          NaN          NaN   \n",
       "24            9.77827       TOR          NaN          NaN          NaN   \n",
       "\n",
       "             left toe 1-conf left toe 2-x left toe 2-y left toe 2-conf  \\\n",
       "frame_number                                                             \n",
       "1                        NaN          NaN          NaN             NaN   \n",
       "9                        NaN          NaN          NaN             NaN   \n",
       "12                       NaN          NaN          NaN             NaN   \n",
       "15                       NaN          NaN          NaN             NaN   \n",
       "24                       NaN          NaN          NaN             NaN   \n",
       "\n",
       "             left heel-x  ... right toe 1-y right toe 1-conf right toe 2-x  \\\n",
       "frame_number              ...                                                \n",
       "1                    NaN  ...           NaN              NaN           NaN   \n",
       "9                    NaN  ...           NaN              NaN           NaN   \n",
       "12                   NaN  ...           NaN              NaN           NaN   \n",
       "15                   NaN  ...           NaN              NaN           NaN   \n",
       "24                   NaN  ...           NaN              NaN           NaN   \n",
       "\n",
       "             right toe 2-y right toe 2-conf right heel-x right heel-y  \\\n",
       "frame_number                                                            \n",
       "1                      NaN              NaN          NaN          NaN   \n",
       "9                      NaN              NaN          NaN          NaN   \n",
       "12                     NaN              NaN          NaN          NaN   \n",
       "15                     NaN              NaN          NaN          NaN   \n",
       "24                     NaN              NaN          NaN          NaN   \n",
       "\n",
       "             right heel-conf treadmill_COP_x treadmill_COP_y  \n",
       "frame_number                                                  \n",
       "1                        NaN             NaN             NaN  \n",
       "9                        NaN             NaN             NaN  \n",
       "12                       NaN             NaN             NaN  \n",
       "15                       NaN             NaN             NaN  \n",
       "24                       NaN             NaN             NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(192, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    3 ... 1511 1512 1513]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-6bdbb04aaf9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msync_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m78\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#sync_log.iloc[idx]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Running'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'video'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mgenerate_frame_and_support_group_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mviz_df_column_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'video'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'completed in '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' seconds.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-2301bd20de7a>\u001b[0m in \u001b[0;36mgenerate_frame_and_support_group_dataframes\u001b[1;34m(index, viz_df_column_names)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mviz_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviz_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#Filling up the time using interpolation (this will indeed follow that each frame is 1/30 seconds apart since our FPS=30)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mPY2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3807\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3808\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3809\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reindex_axis'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4354\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4355\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance, method,\n\u001b[1;32m-> 4356\u001b[1;33m                                   fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[0;32m   4357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4358\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   3739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3740\u001b[0m             frame = frame._reindex_index(index, method, copy, level,\n\u001b[1;32m-> 3741\u001b[1;33m                                          fill_value, limit, tolerance)\n\u001b[0m\u001b[0;32m   3742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3743\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reindex_index\u001b[1;34m(self, new_index, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   3750\u001b[0m         return self._reindex_with_indexers({0: [new_index, indexer]},\n\u001b[0;32m   3751\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3752\u001b[1;33m                                            allow_dups=False)\n\u001b[0m\u001b[0;32m   3753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3754\u001b[0m     def _reindex_columns(self, new_columns, method, copy, level,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4488\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4489\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4490\u001b[1;33m                                                 copy=copy)\n\u001b[0m\u001b[0;32m   4491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   3085\u001b[0m         \u001b[1;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3086\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3087\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3088\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3089\u001b[0m     def reindex(self, target, method=None, level=None, limit=None,\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "#For each video with treadmill and video sync available \n",
    "for idx in range(len(sync_log)):\n",
    "    start_time = time.time()\n",
    "    index = sync_log.loc[78] #sync_log.iloc[idx]\n",
    "    print ('Running', index['video'])\n",
    "    generate_frame_and_support_group_dataframes(index, viz_df_column_names)\n",
    "    print (index['video'], 'completed in ', time.time()-start_time, ' seconds.')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video                GVS_404_W_T1\n",
       "Sync                       Exists\n",
       "total_frame_count            1519\n",
       "cohort                         PD\n",
       "trial                           W\n",
       "scenario                       WT\n",
       "Name: 78, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_log.loc[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
